{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO25/zauk7CaNDqEWUBeRIr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victormurcia/CTS_Test/blob/main/CTS_Unifying_Routines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JZDeKpMyyIv",
        "outputId": "fc1daf46-f017-4fa2-d9ef-972388ef2773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DISTRIB_ID=Ubuntu\n",
            "DISTRIB_RELEASE=20.04\n",
            "DISTRIB_CODENAME=focal\n",
            "DISTRIB_DESCRIPTION=\"Ubuntu 20.04.5 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"20.04.5 LTS (Focal Fossa)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 20.04.5 LTS\"\n",
            "VERSION_ID=\"20.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=focal\n",
            "UBUNTU_CODENAME=focal\n",
            "\n",
            "\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "\n",
            "\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.2/110.2 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.5/575.5 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 KB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for medspacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for medspacy-simstring (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for unqlite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for negspacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.3\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for en-core-sci-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#I need to import locale to ensure that the encoding is set to UTF-8 (weird Google Colab bug)\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "#Check the current build in Google Colab\n",
        "!cat /etc/*release\n",
        "print('\\n')\n",
        "\n",
        "#Check CUDA version\n",
        "!nvcc --version\n",
        "print('\\n')\n",
        "\n",
        "#Ensure that the required packages are installed in the current environment\n",
        "!pip install numpy --quiet\n",
        "!pip install pandas --quiet\n",
        "!pip install spacy==3.4.4 --quiet\n",
        "!pip install scispacy --quiet\n",
        "!pip install medspacy --quiet\n",
        "!pip install negspacy --quiet\n",
        "!pip install transformers\n",
        "!pip install seaborn --quiet\n",
        "!pip install matplotlib --quiet\n",
        "!pip install \"dask[complete]\" --quiet\n",
        "!pip install ipywidgets --quiet\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet \n",
        "print('\\n')\n",
        "\n",
        "#Spacy models used for processing biomedical, scientific, or clinical text \n",
        "#Spacy pipeline for biomedical data.\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the required libraries/packages\n",
        "#General utilities\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dask.dataframe as dd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os, random, time,sys, re\n",
        "from ipywidgets import widgets, interact, interactive, fixed, interact_manual\n",
        "from tqdm import tqdm\n",
        "\n",
        "#NLP Stuff\n",
        "#Spacy\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS #Load stopwords\n",
        "from spacy.language import Language\n",
        "from spacy.tokenizer import Tokenizer\n",
        "#Scispacy\n",
        "import scispacy\n",
        "from scispacy.linking import EntityLinker\n",
        "from scispacy.abbreviation import AbbreviationDetector\n",
        "from scispacy.hyponym_detector import HyponymDetector\n",
        "#Medspacy\n",
        "import medspacy\n",
        "from medspacy.ner import TargetRule\n",
        "from medspacy.visualization import visualize_ent\n",
        "from negspacy.negation import Negex\n",
        "\n",
        "#To use Transformers models from HuggingFace\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel,AutoModelForTokenClassification\n",
        "#NLTK\n",
        "\n",
        "#Enable data to be extracted from my Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSHNWintzPGf",
        "outputId": "fddbe4b2-8404-47a2-9f50-fddbb0423f10"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Patient Dataframe\n",
        "url ='https://raw.githubusercontent.com/victormurcia/CTS_Test/main/test_data/multi_veteran_df.csv'\n",
        "patients_df = pd.read_csv(url)\n",
        "#patients_df\n",
        "\n",
        "#Load the inclusion criteria\n",
        "url ='https://raw.githubusercontent.com/victormurcia/CTS_Test/main/test_data/parsed_ct_ic.csv'\n",
        "parsed_ct_ic = pd.read_csv(url)\n",
        "#parsed_ct_ic\n",
        "\n",
        "#Load the exclusion criteria\n",
        "url ='https://raw.githubusercontent.com/victormurcia/CTS_Test/main/test_data/parsed_ct_ec.csv'\n",
        "parsed_ct_ec = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "31AWBqJ4zUGu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Order of operations\n",
        "# 1. Load the patients dataframe\n",
        "# 2. Extract the EHR for the current patient\n",
        "# 3. Preprocess each patient feature column\n",
        "# 4. Run the NER model on each patient feature\n",
        "# 5. Run a query on clinicaltrials.gov for all the conditions present in the patient profile\n",
        "# 6. Extract the eligibility criteria for each queried clinical trial for each condition\n",
        "# 7. Split eligibility criteria into inclusion/exclusion sections\n",
        "# 8. Run NER model on both inclusion/exclusion sections\n",
        "# 9. Determine Sorensen-Dice index between inclusion/exclusion sections and the patient EHR\n",
        "# 10. Return the complete list of clinical trials stating whether the patient would qualify or not for each clinical trial"
      ],
      "metadata": {
        "id": "kEaIAhQ60YRm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cts_parser(patients_df):\n",
        "  \"\"\"\n",
        "  This function parses the electronic health records for a patient extracted from the Synthetic Veteran Suicide Dataset and a set of Clinical Trials queried using the clinicaltrials.gov API to \n",
        "  determine how good of a match a patient is to a clinical trial. The output of this function will be a dataframe containing a list of clinical trials, their \n",
        "  \"\"\"\n",
        "  #Create and prepare single patient dataframe for NER\n",
        "  final_patient_df = create_patient_df_for_NER(patients_df)\n",
        "\n",
        "  #Load the pre-trained spaCy NER model with sci-spaCy\n",
        "  ss_sm = spacy.load(\"en_core_sci_sm\")\n",
        "\n",
        "  return -1\n",
        "\n",
        "cts_parser(patients_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkOjrbka8ocr",
        "outputId": "2b2c9b17-4f46-432a-ff80-e53222a8ed81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_patient_df_for_NER(patients_df):\n",
        "  \"\"\"\n",
        "  This function creates a dataframe for a single patient from the \n",
        "  \"\"\"\n",
        "  #Select a single row from the DataFrame\n",
        "  row_data = patients_df.loc[0]\n",
        "\n",
        "  #Create a new DataFrame with the single row\n",
        "  patient_df = pd.DataFrame([row_data], columns=row_data.index)\n",
        "\n",
        "  #Select columns that summarize patient profile and put them into a list\n",
        "  allergies     = patient_df['DESCRIPTION_als']\n",
        "  condition     = patient_df['DESCRIPTION_cds']\n",
        "  devices       = patient_df['DESCRIPTION_dvs']\n",
        "  immunizations = patient_df['DESCRIPTION_ims']\n",
        "  medications   = patient_df['DESCRIPTION_mds']\n",
        "  observations  = patient_df['DESCRIPTION_obs']\n",
        "  values        = patient_df['VALUE_obs']\n",
        "  units         = patient_df['UNITS_obs']\n",
        "  procedures    = patient_df['DESCRIPTION_prs']\n",
        "  birthday      = patient_df['BIRTHDATE_pts']\n",
        "  marital       = patient_df['MARITAL_pts']\n",
        "  race          = patient_df['RACE_pts']\n",
        "  ethnicity     = patient_df['ETHNICITY_pts']\n",
        "  gender        = patient_df['GENDER_pts']\n",
        "  city          = patient_df['CITY_pts']\n",
        "  county        = patient_df['COUNTY_pts']\n",
        "\n",
        "  #Make list for patient profile\n",
        "  patient_prof_list = [allergies, condition, devices, immunizations, medications, observations, \n",
        "                       procedures,birthday,marital, race, ethnicity, gender, city, county]\n",
        "  patient_prof_cols = ['allergies', 'condition', 'devices', 'immunizations', \n",
        "                       'medications', 'observations', 'procedures', 'birthday', \n",
        "                       'marital', 'race', 'ethnicity', 'gender', 'city', 'county']\n",
        "\n",
        "  #Create a dictionary with column names and Series data\n",
        "  data_dict = dict(zip(patient_prof_cols, patient_prof_list))\n",
        "\n",
        "  #Create a new DataFrame with the single column\n",
        "  patient_prof = pd.DataFrame(data_dict)\n",
        "\n",
        "  #Concatenate the Series data into a single Series\n",
        "  combined_series = pd.concat(patient_prof_list)\n",
        "\n",
        "  #Create a DataFrame with a single column using the combined Series\n",
        "  final_patient_df = pd.DataFrame({'Patient_Profile': combined_series})\n",
        "\n",
        "  #Get column names of patient df\n",
        "  final_patient_df['aspects'] = patient_prof_cols\n",
        "\n",
        "  #Change the order of columns,reset the index, and drop the index column\n",
        "  final_patient_df = final_patient_df.reindex(columns=['aspects', 'Patient_Profile']).reset_index().drop('index',axis=1)\n",
        "\n",
        "  #Convert the list column to a string column\n",
        "  final_patient_df['Patient_Profile'] = final_patient_df['Patient_Profile'].apply(lambda x: ''.join(map(str, eval(x))))\n",
        "\n",
        "  return final_patient_df\n",
        "\n",
        "def get_umls_codes(text: str):\n",
        "    \n",
        "    #Add the EntityLinker pipe to spacy pipeline\n",
        "    if 'scispacy_linker' not in ss_sm.pipe_names:\n",
        "      ss_sm.add_pipe(\"scispacy_linker\", config={\"linker_name\": \"umls\", \"max_entities_per_mention\": 1})\n",
        "    \n",
        "    # Process the text and extract UMLS codes\n",
        "    doc = ss_sm(text)\n",
        "    umls_codes = [\n",
        "        {\n",
        "            \"text\": entity.text,\n",
        "            #\"start\": entity.start_char,\n",
        "            #\"end\": entity.end_char,\n",
        "            \"umls_id\": umls_ent[0],\n",
        "            \"score\": umls_ent[1]\n",
        "        }\n",
        "        for entity in doc.ents\n",
        "        for umls_ent in entity._.kb_ents\n",
        "    ]\n",
        "    \n",
        "    return umls_codes\n",
        "\n",
        "def extract_values(dicts, key):\n",
        "    return [d.get(key, None) for d in dicts]"
      ],
      "metadata": {
        "id": "3ch2yI0mgvLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}